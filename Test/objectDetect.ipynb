{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d70e1e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04679b62",
   "metadata": {},
   "source": [
    "# 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc908e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cncurrent_directory : c:\\Users\\su1jun\\OneDrive - 서울과학기술대학교\\Capstone\\Capstone\\Test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_directory = os.getcwd()\n",
    "# os.chdir('../'), os.chdir('./python')\n",
    "print(f\"cncurrent_directory : {current_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b34fa8",
   "metadata": {},
   "source": [
    "# 데이터 모델 구축 (학습 > 훈련 > 검증)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7ec15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(current_directory, 'yolov8m.pt')\n",
    "if os.path.exists(model_path):\n",
    "    model = YOLO(model_path) # pass any model type\n",
    "else:\n",
    "    print(\"Not defined file\")\n",
    "\n",
    "# results = model.train(epochs=5)\n",
    "# model.val()  # It'll automatically evaluate the data you trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20b72db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.182  Python-3.11.4 torch-2.0.1+cpu CPU (11th Gen Intel Core(TM) i7-11800H 2.30GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=c:\\Users\\qaxsd\\OneDrive - \\Capstone\\yolov8m.pt, data=c:\\Users\\qaxsd\\OneDrive - \\Capstone\\datasets\\SKU-110K.yaml, epochs=100, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train15\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3776275  ultralytics.nn.modules.head.Detect           [1, [192, 384, 576]]          \n",
      "Model summary: 295 layers, 25856899 parameters, 25856883 gradients\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "WARNING  TensorBoard not initialized correctly, not logging this run. runs\\detect\\train15 is not a directory\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\qaxsd\\OneDrive - 서울과학기술대학교\\datasets\\SKU-110K\\labels... 3512 images, 0 backgrounds, 36 corrupt:  43%|████▎     | 3525/8219 [1:15:08<13:55,  5.62it/s]   "
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(current_directory, 'datasets', 'SKU-110K.yaml')\n",
    "if os.path.exists(data_path):\n",
    "    model.train(data=data_path, epochs=100)\n",
    "else:\n",
    "    print(\"Not defined file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dfc368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters\n",
    "model.tune(data='coco8.yaml', epochs=30, iterations=300, optimizer='AdamW', plots=False, save=False, val=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b5669",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d2f6c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.182  Python-3.11.4 torch-2.0.1+cpu CPU (11th Gen Intel Core(TM) i7-11800H 2.30GHz)\n",
      "YOLOv8m summary (fused): 218 layers, 25886080 parameters, 0 gradients\n",
      "\n",
      "Dataset 'coco.yaml' images not found , missing path 'C:\\Users\\qaxsd\\OneDrive - \\datasets\\coco\\val2017.txt'\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels-segments.zip to 'C:\\Users\\qaxsd\\OneDrive - \\datasets\\coco2017labels-segments.zip'...\n",
      "100%|██████████| 169M/169M [00:48<00:00, 3.64MB/s] \n",
      "Unzipping C:\\Users\\qaxsd\\OneDrive - 서울과학기술대학교\\datasets\\coco2017labels-segments.zip to C:\\Users\\qaxsd\\OneDrive - 서울과학기술대학교\\datasets\\coco...: 100%|██████████| 122232/122232 [02:26<00:00, 831.83file/s] \n",
      "Downloading http://images.cocodataset.org/zips/train2017.zip to 'C:\\Users\\qaxsd\\OneDrive - \\datasets\\coco\\images\\train2017.zip'...\n",
      "Downloading http://images.cocodataset.org/zips/test2017.zip to 'C:\\Users\\qaxsd\\OneDrive - \\datasets\\coco\\images\\test2017.zip'...\n",
      "Downloading http://images.cocodataset.org/zips/val2017.zip to 'C:\\Users\\qaxsd\\OneDrive - \\datasets\\coco\\images\\val2017.zip'...\n"
     ]
    }
   ],
   "source": [
    "# Validate the model\n",
    "metrics = model.val()  # no arguments needed, dataset and settings remembered\n",
    "metrics.box.map    # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps   # a list contains map50-95 of each categorya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf163a5",
   "metadata": {},
   "source": [
    "# 입력값 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "510a34b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 441.8ms\n",
      "Speed: 2.7ms preprocess, 441.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 342.4ms\n",
      "Speed: 4.4ms preprocess, 342.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 318.8ms\n",
      "Speed: 0.0ms preprocess, 318.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 364.2ms\n",
      "Speed: 1.0ms preprocess, 364.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 428.3ms\n",
      "Speed: 0.0ms preprocess, 428.3ms inference, 8.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 379.1ms\n",
      "Speed: 1.7ms preprocess, 379.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 350.9ms\n",
      "Speed: 0.0ms preprocess, 350.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 354.6ms\n",
      "Speed: 0.0ms preprocess, 354.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 349.7ms\n",
      "Speed: 0.0ms preprocess, 349.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 359.5ms\n",
      "Speed: 0.0ms preprocess, 359.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 354.8ms\n",
      "Speed: 3.3ms preprocess, 354.8ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 420.0ms\n",
      "Speed: 0.0ms preprocess, 420.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 427.1ms\n",
      "Speed: 2.0ms preprocess, 427.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 443.4ms\n",
      "Speed: 3.9ms preprocess, 443.4ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 refrigerator, 465.7ms\n",
      "Speed: 2.3ms preprocess, 465.7ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 keyboard, 1 cell phone, 447.4ms\n",
      "Speed: 0.0ms preprocess, 447.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 keyboard, 450.9ms\n",
      "Speed: 0.0ms preprocess, 450.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 414.7ms\n",
      "Speed: 2.0ms preprocess, 414.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 remote, 370.5ms\n",
      "Speed: 0.0ms preprocess, 370.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 378.0ms\n",
      "Speed: 0.0ms preprocess, 378.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 350.6ms\n",
      "Speed: 1.4ms preprocess, 350.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 351.7ms\n",
      "Speed: 0.0ms preprocess, 351.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 1 book, 356.5ms\n",
      "Speed: 2.0ms preprocess, 356.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 1 keyboard, 359.3ms\n",
      "Speed: 0.0ms preprocess, 359.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 laptop, 1 keyboard, 3 books, 362.5ms\n",
      "Speed: 0.0ms preprocess, 362.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 laptop, 1 book, 372.0ms\n",
      "Speed: 2.0ms preprocess, 372.0ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 laptops, 1 keyboard, 1 cell phone, 1 book, 396.0ms\n",
      "Speed: 1.5ms preprocess, 396.0ms inference, 11.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 1 laptop, 1 keyboard, 3 books, 424.0ms\n",
      "Speed: 0.0ms preprocess, 424.0ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 keyboard, 1 book, 400.2ms\n",
      "Speed: 0.0ms preprocess, 400.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 1 laptop, 1 keyboard, 3 books, 365.3ms\n",
      "Speed: 1.6ms preprocess, 365.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 laptop, 1 keyboard, 1 book, 429.5ms\n",
      "Speed: 0.0ms preprocess, 429.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 keyboard, 2 books, 461.3ms\n",
      "Speed: 0.0ms preprocess, 461.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 keyboard, 455.5ms\n",
      "Speed: 0.5ms preprocess, 455.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 remote, 1 toothbrush, 457.0ms\n",
      "Speed: 1.7ms preprocess, 457.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 402.3ms\n",
      "Speed: 0.0ms preprocess, 402.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 3 books, 319.7ms\n",
      "Speed: 2.7ms preprocess, 319.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 2 books, 298.1ms\n",
      "Speed: 0.0ms preprocess, 298.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 2 books, 299.5ms\n",
      "Speed: 0.0ms preprocess, 299.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 2 books, 351.1ms\n",
      "Speed: 1.5ms preprocess, 351.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 4 books, 433.3ms\n",
      "Speed: 0.0ms preprocess, 433.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 426.8ms\n",
      "Speed: 0.0ms preprocess, 426.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 1 laptop, 1 mouse, 1 keyboard, 316.9ms\n",
      "Speed: 0.5ms preprocess, 316.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 1 laptop, 1 mouse, 298.5ms\n",
      "Speed: 1.5ms preprocess, 298.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 1 laptop, 1 keyboard, 297.3ms\n",
      "Speed: 2.0ms preprocess, 297.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 1 laptop, 1 mouse, 299.5ms\n",
      "Speed: 0.0ms preprocess, 299.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 laptop, 314.9ms\n",
      "Speed: 2.2ms preprocess, 314.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 2 laptops, 3 books, 296.1ms\n",
      "Speed: 0.0ms preprocess, 296.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 1 laptop, 3 books, 409.8ms\n",
      "Speed: 1.4ms preprocess, 409.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 1 laptop, 1 keyboard, 3 books, 436.0ms\n",
      "Speed: 2.5ms preprocess, 436.0ms inference, 6.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 tvs, 1 laptop, 1 keyboard, 3 books, 430.0ms\n",
      "Speed: 0.0ms preprocess, 430.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 tvs, 1 laptop, 1 keyboard, 2 books, 444.9ms\n",
      "Speed: 0.0ms preprocess, 444.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 laptop, 1 book, 385.3ms\n",
      "Speed: 0.0ms preprocess, 385.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 1 laptop, 1 mouse, 299.8ms\n",
      "Speed: 1.5ms preprocess, 299.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 2 laptops, 299.4ms\n",
      "Speed: 0.0ms preprocess, 299.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 2 laptops, 332.6ms\n",
      "Speed: 1.9ms preprocess, 332.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 2 laptops, 472.7ms\n",
      "Speed: 0.0ms preprocess, 472.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 1 laptop, 447.8ms\n",
      "Speed: 0.0ms preprocess, 447.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 2 laptops, 1 cell phone, 303.7ms\n",
      "Speed: 0.0ms preprocess, 303.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 1 laptop, 298.1ms\n",
      "Speed: 1.7ms preprocess, 298.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 1 laptop, 295.1ms\n",
      "Speed: 0.0ms preprocess, 295.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 2 laptops, 2 keyboards, 419.8ms\n",
      "Speed: 1.5ms preprocess, 419.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 tvs, 1 laptop, 1 keyboard, 1 cell phone, 433.3ms\n",
      "Speed: 0.0ms preprocess, 433.3ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 1 laptop, 1 mouse, 1 keyboard, 430.0ms\n",
      "Speed: 2.5ms preprocess, 430.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 tvs, 1 laptop, 3 mouses, 2 keyboards, 429.9ms\n",
      "Speed: 2.1ms preprocess, 429.9ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 tvs, 1 laptop, 2 mouses, 1 keyboard, 447.9ms\n",
      "Speed: 1.7ms preprocess, 447.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 tvs, 1 laptop, 3 mouses, 1 keyboard, 471.7ms\n",
      "Speed: 0.0ms preprocess, 471.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 tvs, 1 laptop, 2 mouses, 1 keyboard, 1 cell phone, 402.7ms\n",
      "Speed: 2.1ms preprocess, 402.7ms inference, 8.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 tvs, 1 laptop, 2 mouses, 1 keyboard, 1 cell phone, 303.0ms\n",
      "Speed: 0.0ms preprocess, 303.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 tvs, 1 laptop, 2 mouses, 2 keyboards, 1 cell phone, 306.3ms\n",
      "Speed: 0.0ms preprocess, 306.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 tvs, 1 laptop, 2 mouses, 2 keyboards, 1 cell phone, 334.4ms\n",
      "Speed: 0.0ms preprocess, 334.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 tvs, 1 laptop, 3 mouses, 1 keyboard, 1 cell phone, 439.0ms\n",
      "Speed: 0.0ms preprocess, 439.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 tvs, 1 laptop, 3 mouses, 3 keyboards, 433.3ms\n",
      "Speed: 0.0ms preprocess, 433.3ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 tvs, 1 laptop, 2 mouses, 3 keyboards, 442.5ms\n",
      "Speed: 0.0ms preprocess, 442.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 tvs, 1 laptop, 1 mouse, 1 keyboard, 438.8ms\n",
      "Speed: 0.0ms preprocess, 438.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cell phone, 437.9ms\n",
      "Speed: 2.2ms preprocess, 437.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 laptops, 421.8ms\n",
      "Speed: 0.0ms preprocess, 421.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 laptops, 349.7ms\n",
      "Speed: 0.0ms preprocess, 349.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 laptops, 334.2ms\n",
      "Speed: 0.0ms preprocess, 334.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 laptops, 1 mouse, 1 remote, 435.3ms\n",
      "Speed: 0.0ms preprocess, 435.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 laptops, 1 mouse, 339.5ms\n",
      "Speed: 0.0ms preprocess, 339.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 laptops, 1 mouse, 302.2ms\n",
      "Speed: 0.0ms preprocess, 302.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 laptops, 1 mouse, 1 remote, 295.8ms\n",
      "Speed: 0.0ms preprocess, 295.8ms inference, 10.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 laptops, 1 mouse, 307.0ms\n",
      "Speed: 0.0ms preprocess, 307.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 laptops, 341.2ms\n",
      "Speed: 0.5ms preprocess, 341.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 laptop, 1 book, 319.8ms\n",
      "Speed: 0.0ms preprocess, 319.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 tv, 318.9ms\n",
      "Speed: 0.0ms preprocess, 318.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 clock, 305.9ms\n",
      "Speed: 2.6ms preprocess, 305.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 306.7ms\n",
      "Speed: 2.1ms preprocess, 306.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.namedWindow(\"YOLOv8 Inference\", cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(\"YOLOv8 Inference\", 800, 600)  # 윈도우의 너비와 높이를 800x600으로 설정\n",
    "        cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be099dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process results generator\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bbox outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "\n",
    "# Show the results\n",
    "for r in results:\n",
    "    im_array = r.plot()  # plot a BGR numpy array of predictions\n",
    "    im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\n",
    "    im.show()  # show image\n",
    "    im.save('results.jpg')  # save image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce2a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f6656f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
